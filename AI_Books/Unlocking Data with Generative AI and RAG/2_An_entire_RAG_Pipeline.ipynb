{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to simplify warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "def simple_warning_format(message, category, filename, lineno, file=None, line=None):\n",
    "    return f\"{category.__name__}: {message}\\n\"\n",
    "\n",
    "warnings.formatwarning = simple_warning_format\n",
    "warnings.simplefilter(\"always\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web loading and crawling\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "web_paths=(\"https://kbourne.github.io/chapter1.html\",),\n",
    "bs_kwargs=dict(\n",
    "    parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding and indexing the chunks\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retreiver = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does RAG comare with fine-tuning?\"\n",
    "relevant_docs = retreiver.get_relevant_documents(query)\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promt Templates:\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n",
      "\n",
      "Input Variables: ['context', 'question']\n"
     ]
    }
   ],
   "source": [
    "# Prompt templates from the LangChain Hub\n",
    "\n",
    "prompt = hub.pull(\"jclemens24/rag-prompt\")\n",
    "\n",
    "print(\"Promt Templates:\")\n",
    "print(prompt.messages[0].prompt.template)\n",
    "print(\"\\nInput Variables:\", prompt.input_variables)\n",
    "# print(\"Metadata:\", prompt.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting a function so that it matches the next stepâ€™s input\n",
    "\n",
    "# The purpose of this function is to format the output of the retriever into the string format that it will \n",
    "# need to be in for the next step in the chain, after the retriever step. This function is necessary because\n",
    "# the retriever step outputs a list of documents, but the next step in the chain expects a string.\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining your LLM\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a LangChain chain using LCEL\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retreiver | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "     | prompt\n",
    "     | llm\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The advantages of using Retrieval-Augmented Generation (RAG) include:\\n\\n1. **Improved Accuracy and Relevance**: RAG enhances the accuracy and relevance of responses generated by large language models (LLMs) by incorporating specific, real-time information from databases or datasets.\\n\\n2. **Customization and Flexibility**: RAG allows for tailored responses based on a company's specific needs by integrating internal databases, creating personalized experiences and outputs that meet unique business requirements.\\n\\n3. **Expanding Model Knowledge Beyond Training Data**: RAG enables models to access and utilize information that was not included in their initial training sets, effectively broadening the model's knowledge base without the need for retraining.\\n\\nThese advantages make RAG a powerful tool for organizations looking to leverage their internal data and improve the effectiveness of AI applications.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submitting a question for RAG\n",
    "\n",
    "rag_chain.invoke(\"What are the advantages of using RAG?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
